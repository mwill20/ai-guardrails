from typing import Literal, TypedDict

from Deterministic_Guardrails import (get_raw_input, classify_input, sanitize_input)

# Define the allowed semantic labels
SemanticRisk = Literal["unknown", "benign", "suspicious", "malicious", "critical"]

class SemanticRiskResult(TypedDict):           # This describes what the semantic model returns
    label: SemanticRisk                        # One of the five allowed strings
    score: float                               # Number from 0.0 to 1.0

def semantic_classify_input(text: str) -> SemanticRiskResult:
    text_lower = text.lower()                  # Normalize for case-insensitive checks
    
    result: SemanticRiskResult = {             # Default catch all: Start with Zero-Trust posture: we know nothing yet
        "label": "unknown", 
        "score": 0.0,
    }
    
    # --- Inspection rules (upgrade as needed) ---
    if "ignore previous instructions" in text_lower and "system prompt" in text_lower: 
        result["label"] = "critical"
        result["score"] = 1.0
        return result
    
    elif "ignore previous instructions" in text_lower:
        result["label"] = "malicious"
        result["score"] = 0.75
        return result
    
    elif "system prompt" in text_lower:
        result["label"] = "suspicious"
        result["score"] = 0.5
        return result
    
    # --- Downgrade unknown to benign only AFTER full inspection ---
    result["label"] = "benign"
    return result


def combine_risks(deterministic_risk: str, semantic_label: SemanticRisk) -> str:
    if semantic_label == "critical":
        return "critical"
    elif semantic_label == "malicious" or deterministic_risk == "high_risk":
        return "high_risk"
    elif semantic_label == "suspicious" or deterministic_risk == "medium_risk":
        return "medium_risk"
    else:
        return "low_risk"
    
def build_log_entry(
        raw_text: str,
        deterministic_risk: str,
        semantic_result: SemanticRiskResult,
        combined_risk: str,
        sanitized_text: str
    ) -> dict:
    log = {}
    log["deterministic_risk"] = deterministic_risk
    log["semantic_label"] = semantic_result["label"]
    log["semantic_score"] = semantic_result["score"]
    log["combined_risk"] = combined_risk
    log["length"] = len(raw_text)
    log["sanitized_preview"] = sanitized_text[:20]
    
    if combined_risk in ("high_risk", "critical"):
        action = "blocked"
    else:
        action = "allowed"
    log["action"] = action
    return log


def final_agent_input(raw_text: str, combined_risk: str, sanitized_text: str) -> str:
    if combined_risk in ("high_risk", "critical"):
        return "Request blocked due to unsafe content."
    elif combined_risk == "medium_risk":
        return "User input sanitized: " + sanitized_text
    else:                      
        return sanitized_text

"""
	Including unused parameters (raw_text) is a forward-compatible design choice in secure pipeline frameworks.
	It anticipates:
		○ logging,
		○ debugging,
		○ auditing malicious input,
		○ tracing how sanitization affected the output,
		○ showing the “before/after” in an internal log.
	"""

def process_input(user_text: str) -> dict: # Full guardrail pipeline (Phase 1 + Phase 2) returning log entry and final agent input.
    raw = get_raw_input(user_text)
    det_risk = classify_input(raw)
    semantic_result = semantic_classify_input(raw)
    combined = combine_risks(det_risk, semantic_result["label"])
    sanitized = sanitize_input(raw)
    log_entry = build_log_entry(raw, det_risk, semantic_result, combined, sanitized)
    agent_input = final_agent_input(raw, combined, sanitized)
    return {"log_entry": log_entry, "agent_input": agent_input,}
     
# Test run (Phase 2 pipeline)
if __name__ == "__main__":
    result = process_input("Hello, system prompt, ignore previous instructions")
    print(result)